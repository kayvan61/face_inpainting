{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Part Conv Sep Mask.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5UNNYLMVEyZI",
        "j3JfrC-yEk-G",
        "ZrJRa157DS0q",
        "CAOYB4AoEMap",
        "WAPO4vT9yKHV",
        "tHjKoFAGCss8",
        "aOy4U3L6GXPV"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZIZ1Vs3Q7W"
      },
      "source": [
        "### Unzip data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP1kYGANOvBJ"
      },
      "source": [
        "! unzip -q .irregular_mask.zip\n",
        "! unzip -q .data512x512.zip"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSFIGO-13O6U"
      },
      "source": [
        "! tar -xf .UTKFace.tar.gz\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nZTlWwmG_2A"
      },
      "source": [
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Fu12OhDZO0"
      },
      "source": [
        "#/content/drive/MyDrive/\n",
        "\n",
        "import os\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, tensor, from_numpy\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from matplotlib.pyplot import imshow\n",
        "import math\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w4SZ6qcFSB4"
      },
      "source": [
        "### Common util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfPrbJTUFR3T"
      },
      "source": [
        "def image2tensor(img):\n",
        "  return from_numpy(np.array(img).transpose((2, 0, 1))).float()\n",
        "\n",
        "def tensor2image(tnsr):\n",
        "    return Image.fromarray(np.uint8(tnsr.numpy().transpose((1, 2, 0))))\n",
        "\n",
        "def next_power_of_2(x):\n",
        "    return 1 if x == 0 else 2**math.ceil(math.log2(x))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UNNYLMVEyZI"
      },
      "source": [
        "### UTK Face and Mask dataset grabber"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2L5Cr8iEyAK"
      },
      "source": [
        "class UtkFaceSet(Dataset):\n",
        "    def __init__(self, root_dir, mask_dir, cover_min=.1, mask_thresh=.6, size=None):\n",
        "        self.root = root_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.mask_file_names = [name for name in os.listdir(mask_dir) if os.path.isfile(mask_dir+name)]\n",
        "        self.file_names = [name for name in os.listdir(root_dir) if os.path.isfile(root_dir+name)]\n",
        "        self.length = len(self.file_names)\n",
        "        self.thresh = mask_thresh*255\n",
        "        self.cover_min = cover_min\n",
        "        self.resize = size is not None\n",
        "        self.size = size\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        # get the images\n",
        "        img_name = os.path.join(self.root + self.file_names[idx])\n",
        "        img = Image.open(img_name)\n",
        "        if self.resize is True:\n",
        "          img = img.resize(self.size)\n",
        "        img = img.resize((next_power_of_2(img.size[0]), next_power_of_2(img.size[1])))\n",
        "\n",
        "        # keep adding masks until we hit the threshold\n",
        "        final_mask = np.ones((img.size[0], img.size[1], 3)).astype(np.uint8)\n",
        "        while (((np.count_nonzero(final_mask==0) / (img.width * img.height * 3)))) < self.cover_min:\n",
        "          # get the mask\n",
        "          mask_ind = random.randint(0, len(self.mask_file_names)-1)\n",
        "          mask_img_path = os.path.join(self.mask_dir + self.mask_file_names[mask_ind])\n",
        "\n",
        "          # apply random transforms to the mask\n",
        "          # rotation, dilation, and scaling/cropping\n",
        "          \n",
        "          # binarize the mask\n",
        "          mask_img = cv2.imread(mask_img_path)\n",
        "          ret,mask_img = cv2.threshold(mask_img,self.thresh,255,cv2.THRESH_BINARY)\n",
        "\n",
        "          # rotate the mask\n",
        "          r = [cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_180, cv2.ROTATE_90_COUNTERCLOCKWISE]\n",
        "          mask_img = cv2.rotate(mask_img, r[random.randint(0, len(r)-1)])\n",
        "\n",
        "          # dilate the holes in the mask\n",
        "          filter_size = random.randint(3, 7)\n",
        "          ker = np.ones((filter_size, filter_size))\n",
        "          img_erosion = cv2.erode(mask_img, ker, iterations=1) \n",
        "\n",
        "          # scale\n",
        "          mask_img = cv2.resize(mask_img, (random.randint(int(mask_img.shape[0]*.8), int(mask_img.shape[0]*2)),\n",
        "                                          random.randint(int(mask_img.shape[1]*.8), int(mask_img.shape[1]*2))))\n",
        "          x1 = random.randint(int(mask_img.shape[0]*.2), int(mask_img.shape[0]*.5))\n",
        "          x2 = random.randint(int(mask_img.shape[0]*.6), int(mask_img.shape[0]))\n",
        "          y1 = random.randint(int(mask_img.shape[1]*.2), int(mask_img.shape[1]*.5))\n",
        "          y2 = random.randint(int(mask_img.shape[1]*.6), int(mask_img.shape[1]))\n",
        "          mask_img = mask_img[x1:x2, y1:y2]\n",
        "\n",
        "          try:\n",
        "            mask_img = cv2.resize(mask_img, (img.size[0], img.size[1]))\n",
        "            mask_img = cv2.cvtColor(mask_img, cv2.COLOR_BGR2RGB)\n",
        "          except:\n",
        "            print(img.size)\n",
        "            print(mask_img.shape)\n",
        "\n",
        "          binary_mask = mask_img/255\n",
        "          binary_mask[np.nonzero(binary_mask)] = 1\n",
        "          assert(binary_mask.max() <= 1)\n",
        "          assert(binary_mask.min() >= 0)\n",
        "          final_mask = final_mask & binary_mask.astype(np.uint8)\n",
        "\n",
        "        return {'image': image2tensor(img), \n",
        "                'mask' : image2tensor(final_mask)}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3JfrC-yEk-G"
      },
      "source": [
        "### Model loss module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vXqpdhEEkO3"
      },
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Perceptual Loss Module\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Init\"\"\"\n",
        "        super().__init__()\n",
        "        self.l1_loss = torch.nn.L1Loss()\n",
        "        self.mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_batch(batch, div_factor=255.):\n",
        "        \"\"\"\n",
        "        Normalize batch\n",
        "        :param batch: input tensor with shape\n",
        "         (batch_size, nbr_channels, height, width)\n",
        "        :param div_factor: normalizing factor before data whitening\n",
        "        :return: normalized data, tensor with shape\n",
        "         (batch_size, nbr_channels, height, width)\n",
        "        \"\"\"\n",
        "        # normalize using imagenet mean and std\n",
        "        mean = batch.data.new(batch.data.size())\n",
        "        std = batch.data.new(batch.data.size())\n",
        "        mean[:, 0, :, :] = 0.485\n",
        "        mean[:, 1, :, :] = 0.456\n",
        "        mean[:, 2, :, :] = 0.406\n",
        "        std[:, 0, :, :] = 0.229\n",
        "        std[:, 1, :, :] = 0.224\n",
        "        std[:, 2, :, :] = 0.225\n",
        "        batch = torch.div(batch, div_factor)\n",
        "\n",
        "        batch -= Variable(mean)\n",
        "        batch = torch.div(batch, Variable(std))\n",
        "        return batch\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Forward\n",
        "        :param x: input tensor with shape\n",
        "         (batch_size, nbr_channels, height, width)\n",
        "        :param y: input tensor with shape\n",
        "         (batch_size, nbr_channels, height, width)\n",
        "        :return: l1 loss between the normalized data\n",
        "        \"\"\"\n",
        "        x = self.normalize_batch(x)\n",
        "        y = self.normalize_batch(y)\n",
        "        return self.l1_loss(x, y)\n",
        "\n",
        "def make_vgg16_layers(style_avg_pool = False):\n",
        "    \"\"\"\n",
        "    make_vgg16_layers\n",
        "    Return a custom vgg16 feature module with avg pooling\n",
        "    \"\"\"\n",
        "    vgg16_cfg = [\n",
        "        64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M',\n",
        "        512, 512, 512, 'M', 512, 512, 512, 'M'\n",
        "    ]\n",
        "\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in vgg16_cfg:\n",
        "        if v == 'M':\n",
        "            if style_avg_pool:\n",
        "                layers += [nn.AvgPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrJRa157DS0q"
      },
      "source": [
        "### Partial Convolution Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZRWt8xMC7aI"
      },
      "source": [
        "class PartialConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # whether the mask is multi-channel or not\n",
        "        if 'multi_channel' in kwargs:\n",
        "            self.multi_channel = kwargs['multi_channel']\n",
        "            kwargs.pop('multi_channel')\n",
        "        else:\n",
        "            self.multi_channel = False  \n",
        "\n",
        "        if 'return_mask' in kwargs:\n",
        "            self.return_mask = kwargs['return_mask']\n",
        "            kwargs.pop('return_mask')\n",
        "        else:\n",
        "            self.return_mask = False\n",
        "\n",
        "        super(PartialConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "        if self.multi_channel:\n",
        "            self.weight_maskUpdater = torch.ones(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
        "        else:\n",
        "            self.weight_maskUpdater = torch.ones(1, 1, self.kernel_size[0], self.kernel_size[1])\n",
        "            \n",
        "        self.slide_winsize = self.weight_maskUpdater.shape[1] * self.weight_maskUpdater.shape[2] * self.weight_maskUpdater.shape[3]\n",
        "\n",
        "        self.last_size = (None, None, None, None)\n",
        "        self.update_mask = None\n",
        "        self.mask_ratio = None\n",
        "\n",
        "    def forward(self, input, mask_in=None):\n",
        "        assert len(input.shape) == 4\n",
        "        if mask_in is not None or self.last_size != tuple(input.shape):\n",
        "            self.last_size = tuple(input.shape)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if self.weight_maskUpdater.type() != input.type():\n",
        "                    self.weight_maskUpdater = self.weight_maskUpdater.to(input)\n",
        "\n",
        "                if mask_in is None:\n",
        "                    # if mask is not provided, create a mask\n",
        "                    if self.multi_channel:\n",
        "                        mask = torch.ones(input.data.shape[0], input.data.shape[1], input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                    else:\n",
        "                        mask = torch.ones(1, 1, input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                else:\n",
        "                    mask = mask_in\n",
        "                        \n",
        "                self.update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=1)\n",
        "\n",
        "                # for mixed precision training, change 1e-8 to 1e-6\n",
        "                self.mask_ratio = self.slide_winsize/(self.update_mask + 1e-8)\n",
        "                # self.mask_ratio = torch.max(self.update_mask)/(self.update_mask + 1e-8)\n",
        "                self.update_mask = torch.clamp(self.update_mask, 0, 1)\n",
        "                self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)\n",
        "\n",
        "\n",
        "        raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask) if mask_in is not None else input)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "            output = torch.mul(raw_out - bias_view, self.mask_ratio) + bias_view\n",
        "            output = torch.mul(output, self.update_mask)\n",
        "        else:\n",
        "            output = torch.mul(raw_out, self.mask_ratio)\n",
        "\n",
        "\n",
        "        if self.return_mask:\n",
        "            return output, self.update_mask\n",
        "        else:\n",
        "            return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2Jc3Yj8D3lp"
      },
      "source": [
        "### Down convolution block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raS-fHcuD7iL"
      },
      "source": [
        "class DownPartConvBlock(nn.Module):\n",
        "  def __init__(self, in_planes, planes, kernel=3, stride=1, padding=1, \n",
        "               downsample=None, pooling=nn.MaxPool2d(kernel_size=2, stride=2)):\n",
        "    super(DownPartConvBlock, self).__init__()\n",
        "    self.conv1 = PartialConv2d(in_planes, planes, kernel_size=kernel, stride=stride,\n",
        "                               padding=padding, bias=False, multi_channel=True, \n",
        "                               return_mask=True)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.conv2 = PartialConv2d(planes, planes, kernel_size=kernel, stride=stride,\n",
        "                               padding=padding, bias=False, multi_channel=True, \n",
        "                               return_mask=True)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "    self.pooling = pooling\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "\n",
        "    out, outmask = self.conv1(x, mask)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu1(out)\n",
        "\n",
        "    out, outmask = self.conv2(out, outmask)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu2(out)\n",
        "\n",
        "    out = self.pooling(out)\n",
        "    outmask = self.pooling(outmask)\n",
        "\n",
        "    return out, outmask"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAOYB4AoEMap"
      },
      "source": [
        "### Up convolution block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUHpSaSqEP5Z"
      },
      "source": [
        "class UpPartConvBlock(nn.Module):\n",
        "  def __init__(self, in_planes, planes, kernel=3, stride=1, padding=1, \n",
        "               downsample=None, upsamp=nn.UpsamplingNearest2d(size=None, scale_factor=2)):\n",
        "    super(UpPartConvBlock, self).__init__()\n",
        "    self.conv1 = PartialConv2d(in_planes, planes, kernel_size=kernel, stride=stride,\n",
        "                               padding=padding, bias=False, multi_channel=True, \n",
        "                               return_mask=True)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.conv2 = PartialConv2d(planes, planes, kernel_size=kernel, stride=stride,\n",
        "                               padding=padding, bias=False, multi_channel=True, \n",
        "                               return_mask=True)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "    self.upsamp = upsamp\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "\n",
        "    out, outmask = self.conv1(x, mask)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu1(out)\n",
        "\n",
        "    out, outmask = self.conv2(out, outmask)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu2(out)\n",
        "\n",
        "    if self.upsamp is not None:\n",
        "      out = self.upsamp(out)\n",
        "      outmask = self.upsamp(outmask)\n",
        "\n",
        "    return out, outmask"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAPO4vT9yKHV"
      },
      "source": [
        "### Output layer module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knGp8Az5yLXo"
      },
      "source": [
        "class OutputBlock(nn.Module):\n",
        "  def __init__(self, in_planes, planes, kernel=3, stride=1, padding=1):\n",
        "    super(OutputBlock, self).__init__()\n",
        "    self.conv1 = PartialConv2d(in_planes, planes, kernel_size=kernel, stride=stride,\n",
        "                               padding=padding, bias=False, multi_channel=True, \n",
        "                               return_mask=True)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.conv2 = PartialConv2d(planes, planes, kernel_size=kernel, stride=stride,\n",
        "                               padding=padding, bias=False, multi_channel=True, \n",
        "                               return_mask=True)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.conv1x1 = PartialConv2d(planes, 3, kernel_size=1, stride=1,\n",
        "                                 padding=0, bias=False, multi_channel=True, \n",
        "                                 return_mask=True)\n",
        "  def forward(self, x, mask):\n",
        "\n",
        "    out, outmask = self.conv1(x, mask)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu1(out)\n",
        "\n",
        "    out, outmask = self.conv2(out, outmask)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu2(out)\n",
        "    \n",
        "    out, outmask = self.conv1x1(out, outmask)\n",
        "    return out, outmask"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au8cgt2DESTn"
      },
      "source": [
        "### Model Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FQPLf7gEVaL"
      },
      "source": [
        "class PartConvModel(nn.Module):\n",
        "  def __init__(self, freeze_bn=False):\n",
        "    super(PartConvModel, self).__init__()\n",
        "    \n",
        "    self.freeze_enc_bn = freeze_bn\n",
        "\n",
        "    # encoder layers\n",
        "    self.enc_layer1 = DownPartConvBlock(3, 64)\n",
        "    self.enc_layer2 = DownPartConvBlock(64, 128)\n",
        "    self.enc_layer3 = DownPartConvBlock(128, 256)\n",
        "    self.enc_layer4 = DownPartConvBlock(256, 512)\n",
        "    self.enc_layer5 = DownPartConvBlock(512, 512)\n",
        "    self.enc_layer6 = DownPartConvBlock(512, 512)\n",
        "    self.enc_layer7 = DownPartConvBlock(512, 512)\n",
        "\n",
        "    # decoder layers \n",
        "    self.upsamp1 = nn.UpsamplingNearest2d(size=None, scale_factor=2)\n",
        "    self.layer8 = UpPartConvBlock(1024, 512)\n",
        "    self.layer9 = UpPartConvBlock(1024, 512)\n",
        "    self.layer10 = UpPartConvBlock(1024, 256)\n",
        "    self.layer11 = UpPartConvBlock(512, 128)\n",
        "    self.layer12 = UpPartConvBlock(256, 64)\n",
        "    self.upsamp2 = nn.UpsamplingNearest2d(size=None, scale_factor=2)\n",
        "    self.layer13 = OutputBlock(128, 64)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    \n",
        "    # encode\n",
        "    y1, mask1 = self.enc_layer1(x, mask)\n",
        "    y2, mask2 = self.enc_layer2(y1, mask1)\n",
        "    y3, mask3 = self.enc_layer3(y2, mask2)\n",
        "    y4, mask4 = self.enc_layer4(y3, mask3)\n",
        "    y5, mask5 = self.enc_layer5(y4, mask4)\n",
        "    # 4x4x512\n",
        "    y6, mask6 = self.enc_layer6(y5, mask5)\n",
        "    # 2x2x512\n",
        "    y7, mask7 = self.enc_layer7(y6, mask6)\n",
        "\n",
        "\n",
        "    # decode\n",
        "\n",
        "    x7 = self.upsamp1(y7)\n",
        "    m7 = self.upsamp1(mask7)\n",
        "\n",
        "    x7 = torch.cat([y6, x7], axis=1)\n",
        "    m7 = torch.cat([mask6, m7], axis=1)\n",
        "    y8, mask8 = self.layer8(x7, m7)\n",
        "\n",
        "    x9 = torch.cat([y5, y8], axis=1)\n",
        "    m9 = torch.cat([mask5, mask8], axis=1)\n",
        "    y9, mask9 = self.layer9(x9, m9)\n",
        "\n",
        "    x10 = torch.cat([y4, y9], axis=1)\n",
        "    m10 = torch.cat([mask4, mask9], axis=1)\n",
        "    y10, mask10 = self.layer10(x10, m10)\n",
        "\n",
        "    x11 = torch.cat([y3, y10], axis=1)\n",
        "    m11 = torch.cat([mask3, mask10], axis=1)\n",
        "    y11, mask11 = self.layer11(x11, m11)\n",
        "\n",
        "    x12 = torch.cat([y2, y11], axis=1)\n",
        "    m12 = torch.cat([mask2, mask11], axis=1)\n",
        "    y12, mask12 = self.layer12(x12, m12)\n",
        "\n",
        "    x13 = torch.cat([y1, y12], axis=1)\n",
        "    m13 = torch.cat([mask1, mask12], axis=1)\n",
        "    x13 = self.upsamp2(x13)\n",
        "    m13 = self.upsamp2(m13)\n",
        "    y13, mask13 = self.layer13(x13, m13)\n",
        "\n",
        "    return y13\n",
        "\n",
        "  def train(self, mode=True):\n",
        "        \"\"\"\n",
        "        Override the default train() to freeze the BN parameters\n",
        "        \"\"\"\n",
        "        super().train(mode)\n",
        "        if self.freeze_enc_bn:\n",
        "          for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.BatchNorm2d) and 'enc' in name:\n",
        "              module.eval()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaZp7xjNE9Dn"
      },
      "source": [
        "### Training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pavbQuYqE9ft",
        "outputId": "2eec0de7-30d1-46db-fc03-805d55c23c5b"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "UTKFace_dir = './data512x512/'\n",
        "Mask_dir    = './irregular_mask/disocclusion_img_mask/'\n",
        "\n",
        "num_epochs = 50\n",
        "batch_s = 15\n",
        "learning_rate = 0.0002\n",
        "\n",
        "TPU_Time = False\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"cuda avalible: \", use_cuda)\n",
        "if not TPU_Time:\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "else:\n",
        "  device = xm.xla_device()\n",
        "print(device)\n",
        "model = PartConvModel().to(device)\n",
        "\n",
        "utk_face = UtkFaceSet(UTKFace_dir, Mask_dir, .01, size=(256,256))\n",
        "utk_batch = DataLoader(utk_face, batch_size=batch_s)\n",
        "\n",
        "#criterion = nn.MSELoss(reduction='sum')\n",
        "criterion = PerceptualLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "decayRate = 0.96\n",
        "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda avalible:  True\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRazngFI8L54"
      },
      "source": [
        "# Train the Batch Norms\n",
        "total_step = len(utk_batch)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images) in enumerate(utk_batch):\n",
        "        # Run the forward pass\n",
        "        img_tens  = images['image'].to(device=device, dtype=torch.float)\n",
        "        mask_tens = images['mask' ].to(device=device, dtype=torch.float)\n",
        "\n",
        "        applied_imgs = (img_tens * (mask_tens))\n",
        "\n",
        "        outputs = model(applied_imgs, mask_tens)\n",
        "        \n",
        "        loss = criterion(outputs, img_tens)\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        # Backprop and perform Adam optimisation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if not TPU_Time:\n",
        "          optimizer.step()\n",
        "        else :\n",
        "          xm.optimizer_step(optimizer)\n",
        "          xm.mark_step()\n",
        "\n",
        "        del mask_tens\n",
        "        del img_tens\n",
        "        del applied_imgs\n",
        "        del outputs\n",
        "        del loss\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], avg Loss: {:.4f}, learning rate: {:.4f}'\n",
        "                  .format(epoch + 1, num_epochs, i + 1, total_step, sum(loss_list)/len(loss_list), my_lr_scheduler.get_last_lr()[0]))\n",
        "    my_lr_scheduler.step()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "XIBtWHR1BoN2",
        "outputId": "b9fd7e98-4366-4edc-b869-cb36bb3a8ffb"
      },
      "source": [
        "# freeze the bn to train the pconv layers drop the learning rate too\n",
        "\n",
        "model.freeze_enc_bn = True\n",
        "num_epochs = 100\n",
        "batch_s = 15\n",
        "learning_rate = 0.00005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "total_step = len(utk_batch)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images) in enumerate(utk_batch):\n",
        "        # Run the forward pass\n",
        "        img_tens  = images['image'].to(device=device, dtype=torch.float)\n",
        "        mask_tens = images['mask' ].to(device=device, dtype=torch.float)\n",
        "\n",
        "        applied_imgs = (img_tens * (mask_tens))\n",
        "\n",
        "        outputs = model(applied_imgs, mask_tens)\n",
        "        \n",
        "        loss = criterion(outputs, img_tens)\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        # Backprop and perform Adam optimisation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if not TPU_Time:\n",
        "          optimizer.step()\n",
        "        else :\n",
        "          xm.optimizer_step(optimizer)\n",
        "          xm.mark_step()\n",
        "\n",
        "        del mask_tens\n",
        "        del img_tens\n",
        "        del applied_imgs\n",
        "        del outputs\n",
        "        del loss\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], avg Loss: {:.4f}, learning rate: {:.6f}'\n",
        "                  .format(epoch + 1, num_epochs, i + 1, total_step, sum(loss_list)/len(loss_list), my_lr_scheduler.get_last_lr()[0]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Step [100/2000], avg Loss: 1.9013, learning rate: 0.000026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ecb83d6401d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Backprop and perform Adam optimisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e827BctrFA96"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7R2UCJNFFhD"
      },
      "source": [
        "avg_loss = sum(loss_list)/len(loss_list)\n",
        "model_save_name = \"partConvModelCheckpoint_Loss_{:.4f}.pt\".format(avg_loss)\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)\n",
        "print(path)\n",
        "#/content/gdrive/MyDrive/partConvModelCheckpoint_Loss_0.1923.pt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4jcqIQWqWSZ"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvLbsfOM7kQE"
      },
      "source": [
        "avg_loss = 0.0478\n",
        "model_save_name = \"partConvModelCheckpoint_Loss_{:.4f}.pt\".format(avg_loss)\n",
        "path = F\"/content/gdrive/MyDrive/{model_save_name}\" \n",
        "model.load_state_dict(torch.load(path, map_location=device))\n",
        "vali_dev = torch.device(\"cpu\")\n",
        "model = model.to(vali_dev)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4UDztp0pMM9"
      },
      "source": [
        "vali_face = UtkFaceSet('./UTKFace/', Mask_dir, .10, size=(200,200))\n",
        "vali_batch = DataLoader(vali_face, batch_size=12)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9i5uAZhDTJK"
      },
      "source": [
        "i, image = next(enumerate(vali_batch))\n",
        "in_img_tens = image['image']\n",
        "in_msk_tens = image['mask']\n",
        "in_applied_imgs = (in_img_tens * (in_msk_tens))\n",
        "\n",
        "o_outputs = model(in_applied_imgs.to(vali_dev), in_msk_tens.to(vali_dev)).detach().cpu()\n",
        "o_outputs_maskless = model(in_applied_imgs.to(vali_dev)).detach().cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZxxpjcEK3Pa"
      },
      "source": [
        "batch_ind = 8\n",
        "criterion(o_outputs, in_img_tens).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3my7iPz3F086"
      },
      "source": [
        "imshow(tensor2image(in_applied_imgs[batch_ind]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YWEqzLCGBqM"
      },
      "source": [
        "imshow(tensor2image(o_outputs[batch_ind]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haX6W_wmGI3Q"
      },
      "source": [
        "imshow(tensor2image(o_outputs_maskless[batch_ind]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFXWjdKrsQp0"
      },
      "source": [
        "y1, mask1 = model.layer1(in_applied_imgs.to(vali_dev), in_msk_tens.to(vali_dev))\n",
        "y1 = y1.detach().cpu()\n",
        "mask1 = mask1.detach().cpu()\n",
        "\n",
        "imshow(tensor2image(y1[batch_ind,0:3,:,:]/y1[batch_ind,0:3,:,:].max() * 255))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpVWPJDruogf"
      },
      "source": [
        "imshow((mask1[batch_ind,6]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHjKoFAGCss8"
      },
      "source": [
        "# attempt to clean via post-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZuKmDfXCsIW"
      },
      "source": [
        "sigma_color = 30\n",
        "sigma_space = 30\n",
        "img_to_smooth = tensor2image(o_outputs[batch_ind])\n",
        "img_to_smooth = np.asarray(img_to_smooth)\n",
        "img_to_smooth = cv2.cvtColor(img_to_smooth, cv2.COLOR_RGB2BGR)\n",
        "imshow(cv2.cvtColor(cv2.bilateralFilter(img_to_smooth, 9, sigma_color, sigma_space), cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J23HfQR4FIvf"
      },
      "source": [
        "sigma_color = 30\n",
        "sigma_space = 30\n",
        "img_to_smooth = tensor2image(o_outputs[batch_ind])\n",
        "img_to_smooth = np.asarray(img_to_smooth)\n",
        "img_to_smooth = cv2.cvtColor(img_to_smooth, cv2.COLOR_RGB2BGR)\n",
        "imshow(cv2.cvtColor(cv2.fastNlMeansDenoising(img_to_smooth, h=9), cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOy4U3L6GXPV"
      },
      "source": [
        "# attempt to clean via post-processing maskless"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N3P3mKJGXPV"
      },
      "source": [
        "sigma_color = 30\n",
        "sigma_space = 30\n",
        "img_to_smooth = tensor2image(o_outputs_maskless[batch_ind])\n",
        "img_to_smooth = np.asarray(img_to_smooth)\n",
        "img_to_smooth = cv2.cvtColor(img_to_smooth, cv2.COLOR_RGB2BGR)\n",
        "imshow(cv2.cvtColor(cv2.bilateralFilter(img_to_smooth, 9, sigma_color, sigma_space), cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80QHROJlGXPW"
      },
      "source": [
        "sigma_color = 30\n",
        "sigma_space = 30\n",
        "img_to_smooth = tensor2image(o_outputs_maskless[batch_ind])\n",
        "img_to_smooth = np.asarray(img_to_smooth)\n",
        "img_to_smooth = cv2.cvtColor(img_to_smooth, cv2.COLOR_RGB2BGR)\n",
        "imshow(cv2.cvtColor(cv2.fastNlMeansDenoising(img_to_smooth, h=9), cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}